##########################################################
############### ENVIRONMENT VARIABLES FILE ###############
##########################################################


#--------------------
#-- EXPRESS SERVER --
#--------------------
PORT=3001                                        # port to run the server on
INDEX_HTML_FILE="chat.html"                      # web page to server (chat app)
# INDEX_HTML_FILE="audio.html"                   # web page to server (audio transrciption test)


#----------------------
#-- GENERAL SETTINGS --
#----------------------
MAX_CONCURRENT_REQUESTS_FALLBACK=1               # fallback value for maximum number of concurrent requests (value should be retrieved from LLM server itself, and this value is only used if the value cannot be retrieved)
MAX_RAG_RESULTS=10                               # maximum number of similarity results to return in RAG model
COMPLETED_JOB_CLEANUP_DELAY=300000               # 1000 * 60 * 5 (5 minutes) → delay before completed jobs are removed from redis
INACTIVE_THRESHOLD=10000                         # 1000 * 5 (5 seconds) → interval between inactive client checks
VERBOSE_LOGGING=false                            # whether to enable verbose logging


#----------------------
#-- SEMANTIC ROUTING --
#----------------------
USE_SEMANTIC_ROUTES=false                        # whether to use semantic routes for prompt decision trees
TOP_SEMANTIC_ROUTES=10                           # number of top semantic routes to return when evaluating for prompt decision trees
SEMANTIC_ROUTE_SENSITIVITY=40                    # value within range of 1 and 100 : sensitivity of the semantic route evaluation (higher values are more sensitive)


#------------------
#-- REDIS SERVER --
#------------------
REDIS_HOST=127.0.0.1                             # host of the redis server
REDIS_PORT=6379                                  # port of the redis server


#-------------------------------
#-- CHROMA/EMBEDDING SETTINGS --
#-------------------------------
CHROMA_SERVER_URL="http://127.0.0.1:8001"                      # host of the chroma server
CHROMA_DISTANCE_FUNCTION="cosine"                              # distance function to use for chroma
                                                               # https://docs.trychroma.com/usage-guide#changing-the-distance-function
# ONNX_EMBEDDING_MODEL="all-MiniLM-L6-v2"                      # name of the embedding model to use "all-MiniLM-L6-v2"
# ONNX_EMBEDDING_MODEL="paraphrase-multilingual-MiniLM-L12-v2" # name of the embedding model to use "paraphrase-multilingual-MiniLM-L12-v2"
ONNX_EMBEDDING_MODEL="bge-m3"                                  # name of the embedding model to use "bge-m3" ⇠ !!! requires quantized model
ONNX_EMBEDDING_MODEL_QUANTIZED=true                            # whether to use the quantized version of the embedding model
MIN_CHUNK_TOKEN_SIZE=150                                       # minimum number of tokens in a chunk
MAX_CHUNK_TOKEN_SIZE=300                                       # maximum number of tokens in a chunk
MIN_CHUNK_TOKEN_OVERLAP=10                                     # minimum number of tokens to overlap between chunks
MAX_CHUNK_TOKEN_OVERLAP=30                                     # maximum number of tokens to overlap between chunks
RUN_STARTUP_EMBEDDING_TEST=false                               # whether to run the startup embedding test


#------------------------------------------
#-- LLM SERVER SETTINGS (language model) --
#------------------------------------------
LLM_SERVER_URL="http://127.0.0.1:8080"           # URL of the LLM server
LLM_SERVER_TEMPERATURE=0.1                       # temperature of the LLM server
# LLM_SERVER_GRAMMARS="./grammars/json.gbnf"     # path to the LLM grammars file
LLM_SERVER_GRAMMARS=""                           # path to the LLM grammars file
LLM_SERVER_STOP_TOKENS=["</s>", "<|im_end|>", "<|endoftext|>", "<|user|>", "LLM:", "User:", "USER:"] # stop tokens of the LLM server
# LLM_PROMPT_INSTRUCTIONS="Do not mention \"USER\" in your responses. This is a conversation between USER and LLM, a friendly chatbot. LLM is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision. Use the CONTEXT of this conversation to guide your responses accurately. If you do not know the answer simply answer 'I don't know'.\n\nCONTEXT:\n"
LLM_PROMPT_INSTRUCTIONS="Do not mention \"USER\" in your responses. You are a helpful AI Assistant that follows instructions extremely well.\nUse the following CONTEXT to answer USER questions accurately:\n\nThink step by step before answering the question. You will get a $100 tip if you provide the correct answer.\n\nCONTEXT:\n"
LLM_PREFIX_USER_PROMPT="\nRephrase and respond to the following USER question to arrive at your answer:\n"
# LLM_PREFIX_USER_PROMPT=""


#---------------------------------------------------
#-- WHISPER SERVER SETTINGS (audio transcription) --
#---------------------------------------------------
WHISPER_ENABLED=true                             # whether to enable audio transcriptions via Whisper.cpp server (requires the server to be running)
WHISPER_SERVER_URL="http://127.0.0.1:8087"       # URL of the Whisper.cpp server
