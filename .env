##########################################################
############### ENVIRONMENT VARIABLES FILE ###############
##########################################################


#--------------------
#-- EXPRESS SERVER --
#--------------------
PORT=8080                                        # port to run the server on
INDEX_HTML_FILE="chat.html"                      # web page to server (chat app)
# INDEX_HTML_FILE="audio.html"                   # web page to server (audio transrciption test)


#----------------------
#-- GENERAL SETTINGS --
#----------------------
MAX_CONCURRENT_REQUESTS_FALLBACK=10              # fallback value for maximum number of concurrent requests (value should be
                                                 # retrieved from Llama.cpp server itself; this value is used if there was an issue
                                                 # getting it from Llama.cpp, or another LLM API provider is configured 
                                                 # [LLM_SERVER_API != "llama"])
MAX_RAG_RESULTS=10                               # maximum number of similarity results to return in RAG model
COMPLETED_JOB_CLEANUP_DELAY=300000               # delay before completed jobs are removed from redis (in milliseconds)
INACTIVE_THRESHOLD=10000                         # interval between inactive client checks (in milliseconds)
VERBOSE_LOGGING=true                            # whether to enable verbose logging


#----------------------
#-- SEMANTIC ROUTING --
#----------------------
USE_SEMANTIC_ROUTES=false                        # whether to use semantic routes for prompt decision trees
TOP_SEMANTIC_ROUTES=10                           # number of top semantic routes to return when evaluating for prompt decision trees
SEMANTIC_ROUTE_SENSITIVITY=50                    # value within range of 1 and 100 : sensitivity of the semantic route evaluation 
                                                 # (higher values are more sensitive; 50 is the default median value and wont adjust the 
                                                 # sensitivity of the original route up or down)


#------------------
#-- REDIS SERVER --
#------------------
REDIS_HOST=127.0.0.1                             # host of the redis server
REDIS_PORT=6379                                  # port of the redis server
REDIS_USER="default"                             # username for the redis server
REDIS_PASSWORD="yourpassword"                    # password for the redis server


#-------------------------------
#-- CHROMA/EMBEDDING SETTINGS --
#-------------------------------
# CHROMA_SERVER_URL="http://host.docker.internal:8001"                # host of the chroma server (docker)
CHROMA_SERVER_URL="http://127.0.0.1:8001"                      # host of the chroma server
CHROMA_DISTANCE_FUNCTION="cosine"                              # distance function to use for chroma
                                                               # https://docs.trychroma.com/usage-guide#changing-the-distance-function
# ONNX_EMBEDDING_MODEL="Xenova/all-MiniLM-L6-v2"                      # name of the embedding model to use
# ONNX_EMBEDDING_MODEL="Xenova/bge-m3"                                # name of the embedding model to use "bge-m3" ⇠ requires quantized model
ONNX_EMBEDDING_MODEL="Xenova/paraphrase-multilingual-MiniLM-L12-v2"   # name of the embedding model to use
ONNX_EMBEDDING_MODEL_QUANTIZED=true                                   # whether to use the quantized version of the embedding model
ALLOW_REMOTE_MODELS=true                                              # whether to allow remote models to be downloaded on demand
MIN_CHUNK_TOKEN_SIZE=150                                       # minimum number of tokens in a chunk
MAX_CHUNK_TOKEN_SIZE=300                                       # maximum number of tokens in a chunk
MIN_CHUNK_TOKEN_OVERLAP=10                                     # minimum number of tokens to overlap between chunks
MAX_CHUNK_TOKEN_OVERLAP=30                                     # maximum number of tokens to overlap between chunks


#------------------------------------------
#-- LLM SERVER SETTINGS (language model) --
#------------------------------------------
LLM_SERVER_API="llama"                           #"llama" for llama.cpp or "together" for Together.ai
LLM_SERVER_TEMPERATURE=0.1                       # temperature of the LLM server
LLM_SERVER_STOP_TOKENS=["</s>", "<|im_end|>", "<|endoftext|>", "<|user|>", "LLM:", "User:", "USER:"] # stop tokens of the LLM server

LLAMA_SERVER_URL="http://127.0.0.1:8080"         # URL of the Llama.cpp server

TOGETHER_API_URL="https://api.together.xyz/v1/chat/completions"                     # Together.ai API URL
TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" # Together.ai API key (http://api.together.ai/)
TOGETHER_MODEL="teknium/OpenHermes-2p5-Mistral-7B"              # Together.ai model to use (https://api.together.xyz/models)
TOGETHER_CONTEXT_LENGTH=8000                                    # maximum number of input tokens
TOGETHER_MAX_RESPONSE_TOKENS=500                                # maximum number of response tokens

#~~~~~~~~~~~~~~~~~
#~~ Model Notes ~~
#~~~~~~~~~~~~~~~~~
# model: "teknium/OpenHermes-2p5-Mistral-7B" ⇠ context length 8000 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



#-------------------------
#-- LLM PROMPT SETTINGS --
#-------------------------
LLM_PROMPT_INSTRUCTIONS="Do not mention \"USER\" in your responses. You are a helpful AI Assistant, LLM, that follows instructions extremely well. Use the following CONTEXT to answer USER questions accurately. Think step by step before answering the question. You will get a $100 tip if you provide the correct answer.\n\nCONTEXT "
LLM_PREFIX_USER_PROMPT="\nRephrase and respond to the following USER question to arrive at your answer:\n"


#---------------------------------------------------
#-- WHISPER SERVER SETTINGS (audio transcription) --
#---------------------------------------------------
WHISPER_ENABLED=false                            # whether to enable audio transcriptions via Whisper.cpp server
WHISPER_SERVER_URL="http://127.0.0.1:8087"       # URL of the Whisper.cpp server
