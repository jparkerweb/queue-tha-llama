##########################################################
############### ENVIRONMENT VARIABLES FILE ###############
##########################################################


#--------------------
#-- EXPRESS SERVER --
#--------------------
PORT=8080                                        # port to run the server on
INDEX_HTML_FILE="chat.html"                      # web page to server (chat app)
# INDEX_HTML_FILE="audio.html"                   # web page to server (audio transrciption test)


#----------------------
#-- GENERAL SETTINGS --
#----------------------
MAX_CONCURRENT_REQUESTS_FALLBACK=10              # fallback value for maximum number of concurrent requests (value should be
                                                 # retrieved from Llama.cpp server itself; this value is used if there was an issue
                                                 # getting it from Llama.cpp, or another LLM API provider is configured 
                                                 # [LLM_SERVER_API != "llama"])
MAX_RAG_RESULTS=10                               # maximum number of similarity results to return in RAG model
COMPLETED_JOB_CLEANUP_DELAY=300000               # delay before completed jobs are removed from redis (in milliseconds)
INACTIVE_THRESHOLD=10000                         # interval between inactive client checks (in milliseconds)
VERBOSE_LOGGING=true                             # whether to enable verbose logging


#----------------------
#-- SEMANTIC ROUTING --
#----------------------
USE_SEMANTIC_ROUTES=false                        # whether to use semantic routes for prompt decision trees
TOP_SEMANTIC_ROUTES=10                           # number of top semantic routes to return when evaluating for prompt decision trees
SEMANTIC_ROUTE_SENSITIVITY=50                    # value within range of 1 and 100 : sensitivity of the semantic route evaluation 
                                                 # (higher values are more sensitive; 50 is the default median value and wont adjust the 
                                                 # sensitivity of the original route up or down)


#------------------
#-- REDIS SERVER --
#------------------
# REDIS_HOST="host.docker.internal"              # host of the redis server (docker) 
REDIS_HOST="127.0.0.1"                           # host of the redis server
REDIS_PORT=6379                                  # port of the redis server
REDIS_USER="default"                             # username for the redis server
REDIS_PASSWORD="yourpassword"                    # password for the redis server


#-------------------------------
#-- CHROMA/EMBEDDING SETTINGS --
#-------------------------------
# CHROMA_SERVER_URL="http://host.docker.internal:8001"                # host of the chroma server (docker)
CHROMA_SERVER_URL="http://127.0.0.1:8001"                             # host of the chroma server
CHROMA_DISTANCE_FUNCTION="cosine"                                     # distance function to use for chroma
                                                                      # https://docs.trychroma.com/usage-guide#changing-the-distance-function
# ONNX_EMBEDDING_MODEL="Xenova/all-MiniLM-L6-v2"                      # name of the embedding model to use
# ONNX_EMBEDDING_MODEL="Xenova/bge-m3"                                # name of the embedding model to use "bge-m3" ⇠ requires quantized model
ONNX_EMBEDDING_MODEL="Xenova/paraphrase-multilingual-MiniLM-L12-v2"   # name of the embedding model to use
ONNX_EMBEDDING_MODEL_QUANTIZED=true                                   # whether to use the quantized version of the embedding model
ALLOW_REMOTE_MODELS=true                                              # whether to allow remote models to be downloaded on demand
MIN_CHUNK_TOKEN_SIZE=150                                              # minimum number of tokens in a chunk
MAX_CHUNK_TOKEN_SIZE=300                                              # maximum number of tokens in a chunk
MIN_CHUNK_TOKEN_OVERLAP=10                                            # minimum number of tokens to overlap between chunks
MAX_CHUNK_TOKEN_OVERLAP=30                                            # maximum number of tokens to overlap between chunks


#------------------------------------------
#-- LLM SERVER SETTINGS (language model) --
#------------------------------------------
LLM_SERVER_API='cloud'                                                         # api provider for LLM server: 'llama' or 'cloud'
LLM_SERVER_TEMPERATURE=0                                                       # temperature
#LLM_BASE_URL="http://localhost:11434/v1"                                      # openai api baseurl ⇢ ollama
LLM_BASE_URL="https://api.together.xyz/v1"                                     # openai api baseurl ⇢ together.ai
LLM_API_KEY="xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" # api key (optional if using a local model)
# LLM_MODEL="mistralai/Mixtral-8x7B-Instruct-v0.1"                             # model id <- together.ai -> ["[/INST]","</s>"]
# LLM_MODEL="teknium/OpenHermes-2p5-Mistral-7B"                                # model id
# LLM_MODEL="Llama-3-7B"                                                       # model id <- ollama -> ["<|eot_id|>"]
# LLM_MODEL="Phi-3-mini"                                                       # model id <- ollama -> ["<|user|>", "<|assistant|>", "<|system|>"]
LLM_MODEL="meta-llama/Llama-3-70b-chat-hf"                                     # model id <- together.ai -> ["<|eot_id|>"]
LLM_SERVER_STOP_TOKENS=["<|eot_id|>"]                                          # stop tokens
LLM_CONTEXT_LENGTH=8000                                                        # context length
LLM_MAX_RESPONSE_TOKENS=500                                                    # maximum number of response tokens

#~~~~~~~~~~~~~~~~~
#~~ Model Notes ~~
#~~~~~~~~~~~~~~~~~
# model: "teknium/OpenHermes-2p5-Mistral-7B" ⇠ context length 8000 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



#-------------------------
#-- LLM PROMPT SETTINGS --
#-------------------------
LLM_PROMPT_INSTRUCTIONS="Do not mention \"USER\" in your responses. You are a helpful AI assistant that follows instructions extremely well. Use the following Context to answer USER questions accurately. Think step by step before answering the question. You will get a $100 tip if you provide the correct answer.\n\nContext:\n"
LLM_PREFIX_USER_PROMPT="\nRephrase and respond to the following USER question to arrive at your answer:\n"


#---------------------------------------------------
#-- WHISPER SERVER SETTINGS (audio transcription) --
#---------------------------------------------------
WHISPER_ENABLED=false                            # whether to enable audio transcriptions via Whisper.cpp server
WHISPER_SERVER_URL="http://127.0.0.1:8087"       # URL of the Whisper.cpp server
