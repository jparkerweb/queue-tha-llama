##########################################################
############### ENVIRONMENT VARIABLES FILE ###############
##########################################################


#--------------------
#-- EXPRESS SERVER --
#--------------------
PORT=3001                                        # port to run the server on
INDEX_HTML_FILE="chat.html"                      # web page to server (chat app)
# INDEX_HTML_FILE="audio.html"                   # web page to server (audio transrciption test)


#----------------------
#-- GENERAL SETTINGS --
#----------------------
MAX_CONCURRENT_REQUESTS_FALLBACK=10              # fallback value for maximum number of concurrent requests (value should be
                                                 # retrieved from Llama.cpp server itself; this value is used if there was an issue
                                                 # getting it from Llama.cpp, or another LLM API provider is configured 
                                                 # [LLM_SERVER_API != "llama"])
MAX_RAG_RESULTS=10                               # maximum number of similarity results to return in RAG model
COMPLETED_JOB_CLEANUP_DELAY=300000               # delay before completed jobs are removed from redis (in milliseconds)
INACTIVE_THRESHOLD=10000                         # interval between inactive client checks (in milliseconds)
VERBOSE_LOGGING=false                            # whether to enable verbose logging


#----------------------
#-- SEMANTIC ROUTING --
#----------------------
USE_SEMANTIC_ROUTES=false                        # whether to use semantic routes for prompt decision trees
TOP_SEMANTIC_ROUTES=10                           # number of top semantic routes to return when evaluating for prompt decision trees
SEMANTIC_ROUTE_SENSITIVITY=55                    # value within range of 1 and 100 : sensitivity of the semantic route evaluation 
                                                 # (higher values are more sensitive; 50 is the default median value and wont adjust the 
                                                 # sensitivity of the original route up or down)


#------------------
#-- REDIS SERVER --
#------------------
REDIS_HOST=127.0.0.1                             # host of the redis server
REDIS_PORT=6379                                  # port of the redis server


#-------------------------------
#-- CHROMA/EMBEDDING SETTINGS --
#-------------------------------
CHROMA_SERVER_URL="http://127.0.0.1:8001"                      # host of the chroma server
CHROMA_DISTANCE_FUNCTION="cosine"                              # distance function to use for chroma
                                                               # https://docs.trychroma.com/usage-guide#changing-the-distance-function
# ONNX_EMBEDDING_MODEL="all-MiniLM-L6-v2"                      # name of the embedding model to use
# ONNX_EMBEDDING_MODEL="paraphrase-multilingual-MiniLM-L12-v2" # name of the embedding model to use
ONNX_EMBEDDING_MODEL="bge-m3"                                  # name of the embedding model to use "bge-m3" â‡  requires quantized model
ONNX_EMBEDDING_MODEL_QUANTIZED=true                            # whether to use the quantized version of the embedding model
MIN_CHUNK_TOKEN_SIZE=150                                       # minimum number of tokens in a chunk
MAX_CHUNK_TOKEN_SIZE=300                                       # maximum number of tokens in a chunk
MIN_CHUNK_TOKEN_OVERLAP=10                                     # minimum number of tokens to overlap between chunks
MAX_CHUNK_TOKEN_OVERLAP=30                                     # maximum number of tokens to overlap between chunks


#------------------------------------------
#-- LLM SERVER SETTINGS (language model) --
#------------------------------------------
LLM_SERVER_API="llama"                           #"llama" for llama.cpp or "together" for Together.ai
LLM_SERVER_TEMPERATURE=0.1                       # temperature of the LLM server
LLM_SERVER_STOP_TOKENS=["</s>", "<|im_end|>", "<|endoftext|>", "<|user|>", "LLM:", "User:", "USER:"] # stop tokens of the LLM server

LLAMA_SERVER_URL="http://127.0.0.1:8080"         # URL of the Llama.cpp server

TOGETHER_API_URL="https://api.together.xyz/v1/chat/completions"                     # Together.ai API URL
TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" # Together.ai API key (http://api.together.ai/)
TOGETHER_MODEL="teknium/OpenHermes-2p5-Mistral-7B"                                  # Together.ai model to use (https://api.together.xyz/models)


#-------------------------
#-- LLM PROMPT SETTINGS --
#-------------------------
# LLM_PROMPT_INSTRUCTIONS="Do not mention \"USER\" in your responses. This is a conversation between USER and LLM, a friendly chatbot. LLM is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision. Use the CONTEXT of this conversation to guide your responses accurately. If you do not know the answer simply answer 'I don't know'.\nCONTEXT "
LLM_PROMPT_INSTRUCTIONS="Do not mention \"USER\" in your responses. You are a helpful AI Assistant, LLM, that follows instructions extremely well. Use the following CONTEXT to answer USER questions accurately. Think step by step before answering the question. You will get a $100 tip if you provide the correct answer.\n\nCONTEXT "
LLM_PREFIX_USER_PROMPT="\nRephrase and respond to the following USER question to arrive at your answer:\n"


#---------------------------------------------------
#-- WHISPER SERVER SETTINGS (audio transcription) --
#---------------------------------------------------
WHISPER_ENABLED=false                            # whether to enable audio transcriptions via Whisper.cpp server
WHISPER_SERVER_URL="http://127.0.0.1:8087"       # URL of the Whisper.cpp server
